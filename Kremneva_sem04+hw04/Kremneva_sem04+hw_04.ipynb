{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9f43da11-89a3-46a5-ab9a-c3d05b15dcad",
      "metadata": {
        "id": "9f43da11-89a3-46a5-ab9a-c3d05b15dcad"
      },
      "source": [
        "#### Задача 1.\n",
        "\n",
        "Возьмите любой не очень длинный текст на русском языке (2-3 предложения будет достаточно), хорошо, если это будет текст из соцсетей или любой, с вашей точки зрения, сложный для морфоанализа. Сравните pymorphy и spacy: придется ручками посчитать, сколько у каждого из них в разборах было ошибок."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy2\n",
        "!pip install spacy"
      ],
      "metadata": {
        "id": "CFCPsiMh1af9"
      },
      "id": "CFCPsiMh1af9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pymorphy2\n",
        "import spacy"
      ],
      "metadata": {
        "id": "CbU0Whni1nIe"
      },
      "id": "CbU0Whni1nIe",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download ru_core_news_sm"
      ],
      "metadata": {
        "id": "VuoyCp2nDF49"
      },
      "id": "VuoyCp2nDF49",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3fb3425c-7a72-42d6-b16d-d8b3ebba33ff",
      "metadata": {
        "id": "3fb3425c-7a72-42d6-b16d-d8b3ebba33ff"
      },
      "outputs": [],
      "source": [
        "text = 'Кто инференсит на процах серверных, насколько там производительность меняется между 2 и 12 канальной ОЗУ?) мб есть у кого-то бенчи?)'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# spacy\n",
        "def morph_analysis_spacy(text):\n",
        "    nlp = spacy.load(\"ru_core_news_sm\")\n",
        "    doc = nlp(text)\n",
        "    results = []\n",
        "\n",
        "    for token in doc:\n",
        "        morph_features = token.morph.to_dict()\n",
        "\n",
        "        analysis = {\n",
        "            \"token\": token.text,\n",
        "            \"lemma\": token.lemma_,\n",
        "            \"pos\": token.pos_,\n",
        "            \"tag\": token.tag_,\n",
        "            \"morph\": morph_features  # все морфологические признаки\n",
        "        }\n",
        "        results.append(analysis)\n",
        "\n",
        "    return results\n",
        "\n",
        "results = morph_analysis_spacy(text)\n",
        "for result in results:\n",
        "    print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4uKX8Ms18GS",
        "outputId": "adeaded1-d3fd-491e-e624-6ef47f0143a6"
      },
      "id": "C4uKX8Ms18GS",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'token': 'Кто', 'lemma': 'кто', 'pos': 'PRON', 'tag': 'PRON', 'morph': {'Case': 'Nom'}}\n",
            "{'token': 'инференсит', 'lemma': 'инференсит', 'pos': 'VERB', 'tag': 'VERB', 'morph': {'Aspect': 'Imp', 'Mood': 'Ind', 'Number': 'Sing', 'Person': 'Third', 'Tense': 'Pres', 'VerbForm': 'Fin', 'Voice': 'Act'}}\n",
            "{'token': 'на', 'lemma': 'на', 'pos': 'ADP', 'tag': 'ADP', 'morph': {}}\n",
            "{'token': 'процах', 'lemma': 'процах', 'pos': 'NOUN', 'tag': 'NOUN', 'morph': {'Animacy': 'Anim', 'Case': 'Loc', 'Gender': 'Masc', 'Number': 'Plur'}}\n",
            "{'token': 'серверных', 'lemma': 'серверный', 'pos': 'ADJ', 'tag': 'ADJ', 'morph': {'Case': 'Gen', 'Degree': 'Pos', 'Number': 'Plur'}}\n",
            "{'token': ',', 'lemma': ',', 'pos': 'PUNCT', 'tag': 'PUNCT', 'morph': {}}\n",
            "{'token': 'насколько', 'lemma': 'насколько', 'pos': 'ADV', 'tag': 'ADV', 'morph': {'Degree': 'Pos'}}\n",
            "{'token': 'там', 'lemma': 'там', 'pos': 'ADV', 'tag': 'ADV', 'morph': {'Degree': 'Pos'}}\n",
            "{'token': 'производительность', 'lemma': 'производительность', 'pos': 'NOUN', 'tag': 'NOUN', 'morph': {'Animacy': 'Inan', 'Case': 'Nom', 'Gender': 'Fem', 'Number': 'Sing'}}\n",
            "{'token': 'меняется', 'lemma': 'меняться', 'pos': 'VERB', 'tag': 'VERB', 'morph': {'Aspect': 'Imp', 'Mood': 'Ind', 'Number': 'Sing', 'Person': 'Third', 'Tense': 'Pres', 'VerbForm': 'Fin', 'Voice': 'Mid'}}\n",
            "{'token': 'между', 'lemma': 'между', 'pos': 'ADP', 'tag': 'ADP', 'morph': {}}\n",
            "{'token': '2', 'lemma': '2', 'pos': 'NUM', 'tag': 'NUM', 'morph': {}}\n",
            "{'token': 'и', 'lemma': 'и', 'pos': 'CCONJ', 'tag': 'CCONJ', 'morph': {}}\n",
            "{'token': '12', 'lemma': '12', 'pos': 'NUM', 'tag': 'NUM', 'morph': {}}\n",
            "{'token': 'канальной', 'lemma': 'канальный', 'pos': 'ADJ', 'tag': 'ADJ', 'morph': {'Case': 'Gen', 'Degree': 'Pos', 'Gender': 'Fem', 'Number': 'Sing'}}\n",
            "{'token': 'ОЗУ', 'lemma': 'озу', 'pos': 'PROPN', 'tag': 'PROPN', 'morph': {'Animacy': 'Inan', 'Case': 'Gen', 'Gender': 'Fem', 'Number': 'Sing'}}\n",
            "{'token': '?', 'lemma': '?', 'pos': 'PUNCT', 'tag': 'PUNCT', 'morph': {}}\n",
            "{'token': ')', 'lemma': ')', 'pos': 'PUNCT', 'tag': 'PUNCT', 'morph': {}}\n",
            "{'token': 'мб', 'lemma': 'мб', 'pos': 'ADP', 'tag': 'ADP', 'morph': {}}\n",
            "{'token': 'есть', 'lemma': 'быть', 'pos': 'VERB', 'tag': 'VERB', 'morph': {'Aspect': 'Imp', 'Mood': 'Ind', 'Number': 'Sing', 'Person': 'Third', 'Tense': 'Pres', 'VerbForm': 'Fin', 'Voice': 'Act'}}\n",
            "{'token': 'у', 'lemma': 'у', 'pos': 'ADP', 'tag': 'ADP', 'morph': {}}\n",
            "{'token': 'кого', 'lemma': 'кто', 'pos': 'PRON', 'tag': 'PRON', 'morph': {'Case': 'Gen'}}\n",
            "{'token': '-', 'lemma': '-', 'pos': 'PRON', 'tag': 'PRON', 'morph': {'Case': 'Gen'}}\n",
            "{'token': 'то', 'lemma': 'то', 'pos': 'PRON', 'tag': 'PRON', 'morph': {'Case': 'Gen'}}\n",
            "{'token': 'бенчи', 'lemma': 'бенчи', 'pos': 'NOUN', 'tag': 'NOUN', 'morph': {'Animacy': 'Inan', 'Case': 'Gen', 'Gender': 'Neut', 'Number': 'Sing'}}\n",
            "{'token': '?', 'lemma': '?', 'pos': 'PUNCT', 'tag': 'PUNCT', 'morph': {}}\n",
            "{'token': ')', 'lemma': ')', 'pos': 'PUNCT', 'tag': 'PUNCT', 'morph': {}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pymorphy2\n",
        "# токенизируем с помощью razdel\n",
        "!pip install razdel\n",
        "import re\n",
        "import razdel\n",
        "from razdel import tokenize"
      ],
      "metadata": {
        "id": "KuHGb6DQVwnb"
      },
      "id": "KuHGb6DQVwnb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "morph = pymorphy2.MorphAnalyzer()\n",
        "tokens = [t.text for t in tokenize(text)]\n",
        "\n",
        "for token in tokens:\n",
        "    parse = morph.parse(token)\n",
        "    print(f'TOKEN: {parse[0].word}, LEMMA: {parse[0].normal_form}, MORPH: {parse[0].tag}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9BeeMLy19jz",
        "outputId": "c916b698-0d53-4200-ccf2-aaeed322bbd9"
      },
      "id": "u9BeeMLy19jz",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TOKEN: кто, LEMMA: кто, MORPH: NPRO,masc sing,nomn\n",
            "TOKEN: инференсит, LEMMA: инференсит, MORPH: NOUN,inan,masc sing,nomn\n",
            "TOKEN: на, LEMMA: на, MORPH: PREP\n",
            "TOKEN: процах, LEMMA: проца, MORPH: NOUN,anim,femn plur,loct\n",
            "TOKEN: серверных, LEMMA: серверный, MORPH: ADJF,Qual plur,gent\n",
            "TOKEN: ,, LEMMA: ,, MORPH: PNCT\n",
            "TOKEN: насколько, LEMMA: насколько, MORPH: ADVB,Ques\n",
            "TOKEN: там, LEMMA: там, MORPH: ADVB,Dmns\n",
            "TOKEN: производительность, LEMMA: производительность, MORPH: NOUN,inan,femn sing,nomn\n",
            "TOKEN: меняется, LEMMA: меняться, MORPH: VERB,impf,intr sing,3per,pres,indc\n",
            "TOKEN: между, LEMMA: между, MORPH: PREP\n",
            "TOKEN: 2, LEMMA: 2, MORPH: NUMB,intg\n",
            "TOKEN: и, LEMMA: и, MORPH: CONJ\n",
            "TOKEN: 12, LEMMA: 12, MORPH: NUMB,intg\n",
            "TOKEN: канальной, LEMMA: канальный, MORPH: ADJF femn,sing,gent\n",
            "TOKEN: озу, LEMMA: оз, MORPH: NOUN,inan,masc sing,datv\n",
            "TOKEN: ?, LEMMA: ?, MORPH: PNCT\n",
            "TOKEN: ), LEMMA: ), MORPH: PNCT\n",
            "TOKEN: мб, LEMMA: мб, MORPH: NOUN,inan,masc,Fixd,Abbr plur,gent\n",
            "TOKEN: есть, LEMMA: есть, MORPH: INFN,impf,tran\n",
            "TOKEN: у, LEMMA: у, MORPH: PREP\n",
            "TOKEN: кого-то, LEMMA: кто-то, MORPH: NPRO,masc sing,gent\n",
            "TOKEN: бенчи, LEMMA: бенч, MORPH: NOUN,inan,masc plur,nomn\n",
            "TOKEN: ?, LEMMA: ?, MORPH: PNCT\n",
            "TOKEN: ), LEMMA: ), MORPH: PNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# анализ\n",
        "\n",
        "# Ошибки spacy (12):\n",
        "# для \"инференсит\" - 'lemma': 'инференсит'\n",
        "# для \"процах\" - 'Animacy': 'Anim'\n",
        "# для \"канальной\" - 'Case': 'Gen'\n",
        "# для \"ОЗУ\" - 'pos': 'PROPN', 'tag': 'PROPN', 'Case': 'Gen',\n",
        "# для \"мб\" - 'pos': 'ADP', 'tag': 'ADP'\n",
        "# для \"бенчи\" - lemma': 'бенчи', 'Case': 'Gen', 'Gender': 'Neut', 'Number': 'Sing'\n",
        "\n",
        "\n",
        "# Ошибки pymorphy2 (23):\n",
        "# для \"инференсит\" - LEMMA: инференсит, MORPH: NOUN,inan,masc sing,nomn (не понял даже, что это глагол)\n",
        "# для \"процах\" - LEMMA: проца, MORPH: anim,femn,loct\n",
        "# для \"серверных\" - MORPH: gent\n",
        "# для \"канальной\" - MORPH: gent\n",
        "# для \"озу\" - LEMMA: оз, MORPH: masc,datv\n",
        "# для \"мб\" - MORPH: NOUN,inan,masc,Fixd,Abbr plur,gent\n",
        "# для \"есть\" - LEMMA: есть, MORPH: INFN,tran (грустно)\n",
        "\n",
        "\n",
        "\n",
        "# Мне нравится, что у pymorphy2 так: TOKEN: кого-то, LEMMA: кто-то, MORPH: NPRO,masc sing,gent, а spacy разделил на КТО и ТО\n",
        "# Еще pymorphy2 справился с бенчем! однако инференсить стал существительным\n",
        "# У обоих ожидаемые проблемки с  мб, процессорами и озу"
      ],
      "metadata": {
        "id": "IP0QeekH5_OE"
      },
      "id": "IP0QeekH5_OE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "23550d88-cc6f-4a67-b0fa-64858a0bf5f3",
      "metadata": {
        "id": "23550d88-cc6f-4a67-b0fa-64858a0bf5f3"
      },
      "source": [
        "#### Задача 2.\n",
        "\n",
        "Для любого (достаточно длинного - возьмите файл!) текста на русском языке посчитайте процентный состав по частям речи: сколько в нем в процентах сущ, гл и т.п."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "193116b5-7941-4eb2-870e-91c6598f1cc5",
      "metadata": {
        "id": "193116b5-7941-4eb2-870e-91c6598f1cc5"
      },
      "outputs": [],
      "source": [
        "with open('/content/последний_лист.txt', encoding = 'utf8') as f:\n",
        "    my_text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def pos_counter(text):\n",
        "    nlp = spacy.load(\"ru_core_news_sm\")\n",
        "    doc = nlp(text)\n",
        "\n",
        "    all_pos = [token.pos_ for token in doc] # соберем в список части речи всех слов\n",
        "    statistics_dict = Counter(all_pos) # посчитаем статистику\n",
        "\n",
        "    return dict(statistics_dict), all"
      ],
      "metadata": {
        "id": "YDZlIGshf1x1"
      },
      "id": "YDZlIGshf1x1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d = pos_counter(my_text)\n",
        "d"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJmQp3WMf_VM",
        "outputId": "3c3b60c8-a26d-4779-92d4-21e577e6d2d1"
      },
      "id": "SJmQp3WMf_VM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'PROPN': 96,\n",
              " 'SPACE': 55,\n",
              " 'ADJ': 203,\n",
              " 'NOUN': 425,\n",
              " 'ADP': 198,\n",
              " 'VERB': 343,\n",
              " 'CCONJ': 85,\n",
              " 'PUNCT': 510,\n",
              " 'DET': 61,\n",
              " 'NUM': 50,\n",
              " 'ADV': 134,\n",
              " 'PART': 88,\n",
              " 'PRON': 187,\n",
              " 'SCONJ': 55,\n",
              " 'AUX': 25,\n",
              " 'INTJ': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all = sum([v for k, v in d.items() if k not in ['SPACE', 'PUNCT', 'NUM']])\n",
        "\n",
        "for k,v in d.items():\n",
        "    if k not in ['SPACE', 'PUNCT', 'NUM']:\n",
        "        print(f'{k}: {v/all*100:.2f} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6MUp-k07iXij",
        "outputId": "669476c6-7deb-46a3-8dbf-37b2d3e96d49"
      },
      "id": "6MUp-k07iXij",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PROPN: 5.05 %\n",
            "ADJ: 10.68 %\n",
            "NOUN: 22.36 %\n",
            "ADP: 10.42 %\n",
            "VERB: 18.04 %\n",
            "CCONJ: 4.47 %\n",
            "DET: 3.21 %\n",
            "ADV: 7.05 %\n",
            "PART: 4.63 %\n",
            "PRON: 9.84 %\n",
            "SCONJ: 2.89 %\n",
            "AUX: 1.32 %\n",
            "INTJ: 0.05 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a8a5482-91af-4286-b9be-897e9ec5114b",
      "metadata": {
        "id": "8a8a5482-91af-4286-b9be-897e9ec5114b"
      },
      "source": [
        "#### Задача 3.\n",
        "\n",
        "Посчитайте, сколько раз в тексте прилагательное шло после существительного, а сколько раз - до него (т.е. все случаи типа \"красивая девочка\" vs \"девочка красивая\"). Подумайте, можно ли доверять этим цифрам."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"ru_core_news_sm\")\n",
        "doc = nlp(my_text)"
      ],
      "metadata": {
        "id": "JAwHRxCuqFcI"
      },
      "id": "JAwHRxCuqFcI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1c42e8a-8baf-4205-b496-006c09acac0f",
      "metadata": {
        "id": "f1c42e8a-8baf-4205-b496-006c09acac0f"
      },
      "outputs": [],
      "source": [
        "adj_noun = 0\n",
        "adj_noun_list = []\n",
        "noun_adj = 0\n",
        "noun_adj_list = []\n",
        "\n",
        "# используем функцию из первой задачи\n",
        "my_list = morph_analysis_spacy(my_text)\n",
        "\n",
        "# создадим все возможные пары слов, идущих друг за другом\n",
        "pairs = []\n",
        "for i in range(len(my_list) - 1):\n",
        "    pairs.append((my_list[i], my_list[i + 1]))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# теперь можно работать с парами\n",
        "# будем проверять главное условие (прилаг-сущ / сущ-прилаг) + совпадение рода и числа\n",
        "for s in pairs:\n",
        "    s = list(s)\n",
        "    if s[0]['pos'] in ['NOUN', 'PROPN'] and s[1]['pos'] == 'ADJ':\n",
        "        noun_adj_list.append(s)\n",
        "        noun_adj += 1\n",
        "    elif s[0]['pos'] == 'ADJ' and s[1]['pos'] in ['NOUN', 'PROPN']:\n",
        "        adj_noun_list.append(s)\n",
        "        adj_noun += 1"
      ],
      "metadata": {
        "id": "n0MKFQIBuuIO"
      },
      "id": "n0MKFQIBuuIO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Доверять этим цифрам, конечно, нельзя\n",
        "# Но можно, например, проверять в каждой паре совпадение рода, числа - так уже будет посимпатичнее\n",
        "# но там будет сложность с тем, что не для каждого слова все эти признаки будут поределены, так что придется еще это учитывать\n",
        "print(f'сущ-прилаг: {noun_adj}')\n",
        "print(f'прилаг-сущ: {adj_noun}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dHOIpXniupDj",
        "outputId": "486d4bc8-dd86-4e3d-e72d-040de9a526bc"
      },
      "id": "dHOIpXniupDj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "сущ-прилаг: 21\n",
            "прилаг-сущ: 127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# попробуем\n",
        "new_adj_noun = 0\n",
        "new_adj_noun_list = []\n",
        "new_noun_adj_list = []\n",
        "new_noun_adj = 0\n",
        "\n",
        "for s in pairs:\n",
        "    s = list(s)\n",
        "    # проверяем наличие нужных ключей в morph\n",
        "    gender_0 = s[0]['morph'].get('Gender')\n",
        "    number_0 = s[0]['morph'].get('Number')\n",
        "    gender_1 = s[1]['morph'].get('Gender')\n",
        "    number_1 = s[1]['morph'].get('Number')\n",
        "\n",
        "    # если все ключи существуют, сравниваем\n",
        "    if (gender_0 is not None and number_0 is not None and\n",
        "            gender_1 is not None and number_1 is not None and\n",
        "            s[0]['pos'] in ['NOUN', 'PROPN'] and s[1]['pos'] == 'ADJ' and\n",
        "            gender_0 == gender_1 and number_0 == number_1):\n",
        "\n",
        "        new_noun_adj_list.append(s)\n",
        "        new_noun_adj += 1\n",
        "\n",
        "    elif (gender_0 is not None and number_0 is not None and\n",
        "            gender_1 is not None and number_1 is not None and\n",
        "            s[0]['pos'] == 'ADJ' and s[1]['pos'] in ['NOUN', 'PROPN'] and\n",
        "            gender_0 == gender_1 and number_0 == number_1):\n",
        "\n",
        "        new_adj_noun_list.append(s)\n",
        "        new_adj_noun += 1"
      ],
      "metadata": {
        "id": "rVSivuIT3sTd"
      },
      "id": "rVSivuIT3sTd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# почистили, должно стать лучше\n",
        "print(f'сущ-прилаг: {new_noun_adj}') # было 21\n",
        "print(f'прилаг-сущ: {new_adj_noun}') # было 127"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fpF6ffA14U0s",
        "outputId": "192cb0d6-6eda-4a91-ad74-c55ae8c8e302"
      },
      "id": "fpF6ffA14U0s",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "сущ-прилаг: 4\n",
            "прилаг-сущ: 91\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for s in new_adj_noun_list:\n",
        "    s = list(s)\n",
        "    print(s[0]['token'], s[1]['token'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEAlI_oH4jQt",
        "outputId": "a8b08445-9b19-42aa-b77f-0727495e20ab"
      },
      "id": "UEAlI_oH4jQt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Последний лист\n",
            "небольшом квартале\n",
            "ценное свойство\n",
            "единого цента\n",
            "квартирной платы\n",
            "своеобразный квартал\n",
            "кирпичного дома\n",
            "Восьмой улице\n",
            "цикорный салат\n",
            "общая студия\n",
            "неприветливый чужак\n",
            "старым джентльменом\n",
            "Миниатюрная девушка\n",
            "достойным противником\n",
            "железной кровати\n",
            "мелкий переплет\n",
            "голландского окна\n",
            "глухую стену\n",
            "кирпичного дома\n",
            "озабоченный доктор\n",
            "маленькая барышня\n",
            "Неаполитанский залив\n",
            "губная гармоника\n",
            "похоронной процессии\n",
            "целебной силы\n",
            "бумажную салфеточку\n",
            "чертежной доской\n",
            "журнальному рассказу\n",
            "тихий шепот\n",
            "обратном порядке\n",
            "унылый двор\n",
            "глухая стена\n",
            "кирпичного дома\n",
            "старый плющ\n",
            "кирпичную стену\n",
            "Холодное дыхание\n",
            "последний лист\n",
            "Первый раз\n",
            "великолепным презрением\n",
            "гадкая девочка\n",
            "нового дома\n",
            "больной девочки\n",
            "последний лист\n",
            "нужен свет\n",
            "другой комнате\n",
            "поверженная статуя\n",
            "последний лист\n",
            "нижнем этаже\n",
            "подобной мазни\n",
            "всякой сентиментальностью\n",
            "сторожевого пса\n",
            "полутемной каморке\n",
            "нижнего этажа\n",
            "нетронутое полотно\n",
            "непрочная связь\n",
            "Первый раз\n",
            "маленькая мисс\n",
            "противный старик\n",
            "настоящая женщина\n",
            "хорошей девушке\n",
            "самого подоконника\n",
            "другую комнату\n",
            "старый плющ\n",
            "синей рубашке\n",
            "другое утро\n",
            "короткого сна\n",
            "зеленой шторы\n",
            "проливного дождя\n",
            "кирпичной стене\n",
            "усталой головой\n",
            "далекий путь\n",
            "Болезненная фантазия\n",
            "одинокий лист\n",
            "кирпичной стены\n",
            "северный ветер\n",
            "голландской кровли\n",
            "беспощадная Джонси\n",
            "куриный бульон\n",
            "газовой горелке\n",
            "скверной девчонкой\n",
            "последний лист\n",
            "Неаполитанский залив\n",
            "хорошем уходе\n",
            "другой день\n",
            "бесполезный шарф\n",
            "белая мышка\n",
            "первого дня\n",
            "бедного старика\n",
            "ужасную ночь\n",
            "последний лист\n",
            "последний лист\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# прилаг-сущ получилось красивое, а сущ-прилаг нет, ну оно и ясно, падеж еще нало было учесть\n",
        "for s in new_noun_adj_list:\n",
        "    s = list(s)\n",
        "    print(s[0]['token'], s[1]['token'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSnEV9uw5Kvn",
        "outputId": "5ac14316-b26a-40f5-e38f-ff376dbd042a"
      },
      "id": "FSnEV9uw5Kvn",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ноябре неприветливый\n",
            "половины кирпичную\n",
            "Шел холодный\n",
            "болезни тяжелая\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ccebc92c-3318-4c1e-8be8-be1ea577fd7f",
      "metadata": {
        "id": "ccebc92c-3318-4c1e-8be8-be1ea577fd7f"
      },
      "source": [
        "#### Задача 4 (2 балла).\n",
        "\n",
        "В корпусной лингвистике и NLP существует такая метрика, как [TF-IDF](https://ru.wikipedia.org/wiki/TF-IDF). Соберите игрушечный корпус на 10 текстов (необязательно больших); напишите программу, которая будет по запросу пользователя выводить эту метрику для заданного пользователем слова на вашем корпусе. Это означает, что корпус необходимо будет лемматизировать и посчитать количество слов по документам.\n",
        "\n",
        "Подсказка: функции для подсчета логарифмов есть в модуле math."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "id": "V5A_YmOV1Xjm"
      },
      "id": "V5A_YmOV1Xjm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# корпус\n",
        "df = pd.read_csv(r'/content/my_corpus.txt', sep='|')\n",
        "corpus = df['text'].tolist()\n",
        "print(len(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMJSHdnG2c-e",
        "outputId": "83940d83-ee2b-4b99-b979-a776ffcf831c"
      },
      "id": "IMJSHdnG2c-e",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e9efcf4-709c-49e7-8edd-438369ca5e9d",
      "metadata": {
        "id": "3e9efcf4-709c-49e7-8edd-438369ca5e9d"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "!python -m spacy download ru_core_news_sm\n",
        "nlp = spacy.load(\"ru_core_news_sm\")\n",
        "\n",
        "def corpus_lemmatizer(document):\n",
        "    doc = nlp(document)\n",
        "\n",
        "    lemmas = [token.lemma_ for token in doc] # получаем леммы\n",
        "\n",
        "    return lemmas\n",
        "\n",
        "lemmatized_corpus = [corpus_lemmatizer(doc) for doc in corpus]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from typing import Dict\n",
        "\n",
        "def count_words_in_documents(corpus) :\n",
        "    # подсчитываем кол-во вхождений каждого слова в каждом документе\n",
        "    # возвращает словарь, где ключи — уникальные слова, а значения — словари, содержащие номера документов и кол-во вхождений данного слова в них\n",
        "\n",
        "    word_counts: Dict[str, Dict[int, int]] = {}\n",
        "\n",
        "    for i, document in enumerate(corpus):\n",
        "        unique_words = set(document)\n",
        "        for word in unique_words:\n",
        "            if word not in word_counts:\n",
        "                word_counts[word] = {i: 1}\n",
        "            else:\n",
        "                if i not in word_counts[word]:\n",
        "                    word_counts[word][i] = 1\n",
        "                else:\n",
        "                    word_counts[word][i] += 1\n",
        "\n",
        "    return word_counts\n",
        "\n",
        "statistics = count_words_in_documents(lemmatized_corpus)"
      ],
      "metadata": {
        "id": "ULbQmQ-52Tks"
      },
      "id": "ULbQmQ-52Tks",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# функция для расчета TF-IDF\n",
        "def tf_idf_func(word, statistics, total_docs=len(corpus)):\n",
        "    # будем здесь хранить результаты - значения метрики для каждого документа\n",
        "    tf_values = []\n",
        "    # логарифм отношения общего кол-ва документов к кол-ву документов, содержащих данное слово\n",
        "    # прибавляю 1, чтобы вдруг не случилось деления на ноль\n",
        "    idf_value = math.log(total_docs / sum(1 for c in statistics[word].values() if c > 0)) + 1\n",
        "\n",
        "    for doc_index in range(total_docs):\n",
        "        # получаем кол-во вхождений слова в текущем документе. Если слово отсутствует, считаем его равным нулю\n",
        "        term_count = statistics[word].get(doc_index, 0)\n",
        "        # отношение кол-ва вхождений слова к общему кол-ву слов в документе\n",
        "        tf = term_count / len(lemmatized_corpus[doc_index])\n",
        "        # перемножаем и добавляем полученное значение в список\n",
        "        tf_values.append(tf * idf_value)\n",
        "\n",
        "    return tf_values"
      ],
      "metadata": {
        "id": "d72HBFRAGNBE"
      },
      "id": "d72HBFRAGNBE",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# тестим\n",
        "my_word = input(\"Введите слово: \").lower()\n",
        "tf_idf_values = tf_idf_func(my_word, statistics)\n",
        "for i, value in enumerate(tf_idf_values):\n",
        "    print(f\"TF-IDF для слова '{my_word}' в документе {i+1}: {value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJfshPTJIa8-",
        "outputId": "9f6630e8-37ab-42bd-ad7d-5fc5e4394555"
      },
      "id": "xJfshPTJIa8-",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Введите слово: платформа\n",
            "TF-IDF для слова 'платформа' в документе 1: 0.0105\n",
            "TF-IDF для слова 'платформа' в документе 2: 0.0000\n",
            "TF-IDF для слова 'платформа' в документе 3: 0.0000\n",
            "TF-IDF для слова 'платформа' в документе 4: 0.0000\n",
            "TF-IDF для слова 'платформа' в документе 5: 0.0000\n",
            "TF-IDF для слова 'платформа' в документе 6: 0.0000\n",
            "TF-IDF для слова 'платформа' в документе 7: 0.0000\n",
            "TF-IDF для слова 'платформа' в документе 8: 0.0000\n",
            "TF-IDF для слова 'платформа' в документе 9: 0.0000\n",
            "TF-IDF для слова 'платформа' в документе 10: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ДЗ Задача 1 (5 баллов).\n",
        "\n",
        "Некоторые предлоги в русском языке могут управлять разными падежами (например, \"я еду в Лондон\" vs \"я живу в Лондоне\"). Давайте проанализируем эти предлоги и их падежи. Необходимо:\n",
        "\n",
        "*   составить список таких предлогов (РГ-80 вам в помощь)\n",
        "*   взять достаточно большой текст (можно большое художественное произведение)\n",
        "*   сделать морфоразбор этого текста (лучше не pymorphy)\n",
        "*   Посчитать, как часто и какие падежи встречаются у слова, идущего после предлога.\n",
        "\n",
        "Примечания: во-первых, имейте в виду, что иногда после предлога могут идти самые неожиданные вещи: \"я что, должен ехать на, черт побери, северный полюс?\". Во-вторых, неплохо бы учитывать отсутствие пунктуации (конечно, в норме, как нам кажется, предлог обязательно требует зависимое, но! \"да иди ты на!\") Эти штуки можно отсеять, если просто учитывать только заранее определенные падежи, а не считать все, какие встретились (так и None можно огрести)."
      ],
      "metadata": {
        "id": "w_MfYzfuroA-"
      },
      "id": "w_MfYzfuroA-"
    },
    {
      "cell_type": "code",
      "source": [
        "# ПО - Д.п., В.п., Пр.п. (по приезде, тосковать по нас)\n",
        "# ЗА — В.п., Тв.п.\n",
        "# НА и В — В.п., Пр.п.\n",
        "# С — Р.п., В.п., Тв.п."
      ],
      "metadata": {
        "id": "zDMky5ENWA23"
      },
      "id": "zDMky5ENWA23",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/Straud_Agentstvo_Lokvud_i_Kompaniya_1_Krichashchaya_lestnica_RuLit.txt', encoding='utf8') as f:\n",
        "    textt = f.read()"
      ],
      "metadata": {
        "id": "0IeaZP5hrqRw"
      },
      "id": "0IeaZP5hrqRw",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# используем функцию из первой задачи\n",
        "my_listt = morph_analysis_spacy(textt)\n",
        "\n",
        "# создадим все возможные пары слов, идущих друг за другом\n",
        "pairss = []\n",
        "for i in range(len(my_listt) - 1):\n",
        "    pairss.append((my_listt[i], my_listt[i + 1]))"
      ],
      "metadata": {
        "id": "wbi8jiD1f9SD"
      },
      "id": "wbi8jiD1f9SD",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import name\n",
        "# сбор статистики\n",
        "po = {'Dat': [], 'Acc': [], 'Loc': []}\n",
        "za = {'Acc': [], 'Ins': []}\n",
        "na = {'Acc': [], 'Loc': []}\n",
        "v = {'Acc': [], 'Loc': []}\n",
        "ss = {'Gen': [], 'Acc': [], 'Ins': []}\n",
        "\n",
        "for p in pairss:\n",
        "    p = list(p)\n",
        "    if p[0]['pos'] == 'ADP' and p[1]['pos'] in ['NOUN', 'PROPN']:\n",
        "        if p[1]['morph'].get('Case') is not None:\n",
        "            if p[0]['lemma'] == 'по' and p[1]['morph']['Case'] in ['Dat', 'Acc', 'Loc']:\n",
        "                po[p[1]['morph']['Case']].append(p[1]['token'])\n",
        "            elif p[0]['lemma'] == 'за' and p[1]['morph']['Case'] in ['Acc', 'Ins']:\n",
        "                za[p[1]['morph']['Case']].append(p[1]['token'])\n",
        "            elif p[0]['lemma'] == 'на' and p[1]['morph']['Case'] in ['Acc', 'Loc']:\n",
        "                na[p[1]['morph']['Case']].append(p[1]['token'])\n",
        "            elif p[0]['lemma'] == 'в' and p[1]['morph']['Case'] in ['Acc', 'Loc']:\n",
        "                v[p[1]['morph']['Case']].append(p[1]['token'])\n",
        "            elif p[0]['lemma'] == 'с' and p[1]['morph']['Case'] in ['Gen', 'Acc', 'Ins']:\n",
        "                ss[p[1]['morph']['Case']].append(p[1]['token'])"
      ],
      "metadata": {
        "id": "Tt2_S2y7gv5F"
      },
      "id": "Tt2_S2y7gv5F",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# выведем статистику\n",
        "print(f\"Статистика для предлога ПО: Dat - {len(po['Dat'])}, Acc - {len(po['Acc'])}, Loc - {len(po['Loc'])}.\")\n",
        "print(f\"Статистика для предлога ЗА: Acc - {len(za['Acc'])}, Ins - {len(za['Ins'])}.\")\n",
        "print(f\"Статистика для предлога НА: Acc - {len(na['Acc'])}, Loc - {len(na['Loc'])}.\")\n",
        "print(f\"Статистика для предлога В: Acc - {len(v['Acc'])}, Loc - {len(v['Loc'])}.\")\n",
        "print(f\"Статистика для предлога С: Gen - {len(ss['Gen'])}, Acc - {len(ss['Acc'])}, Ins - {len(ss['Ins'])}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yCFmthupwovF",
        "outputId": "a68d7817-53ea-4ef3-b611-4f10581b9c0a"
      },
      "id": "yCFmthupwovF",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Статистика для предлога ПО: Dat - 64, Acc - 1, Loc - 0.\n",
            "Статистика для предлога ЗА: Acc - 22, Ins - 23.\n",
            "Статистика для предлога НА: Acc - 89, Loc - 95.\n",
            "Статистика для предлога В: Acc - 93, Loc - 160.\n",
            "Статистика для предлога С: Gen - 20, Acc - 1, Ins - 77.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(po['Dat'])\n",
        "print(po['Acc'])\n",
        "print(po['Loc'])\n",
        "print(za['Acc'])\n",
        "print(za['Ins'])\n",
        "print(na['Acc'])\n",
        "print(na['Loc'])\n",
        "print(v['Acc'])\n",
        "print(v['Loc'])\n",
        "print(ss['Gen'])\n",
        "print(ss['Acc'])\n",
        "print(ss['Ins'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HlKWo2K0QM_",
        "outputId": "755827bb-d0df-4984-e126-a11373c8ea98"
      },
      "id": "3HlKWo2K0QM_",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['делам', 'ночам', 'свету', 'дорожке', 'Шин', 'дорожке', 'ступенькам', 'эфесу', 'сторонам', 'телефону', 'сторонам', 'чашечке', 'полу', 'морю', 'телефону', 'лестницам', 'дороге', 'столу', 'лестнице', 'ступенькам', 'форме', 'сути', 'холлу', 'лестнице', 'щиколотки', 'воздуху', 'металлу', 'полу', 'высоте', 'доске', 'полу', 'комнате', 'сторонам', 'комнате', 'плану', 'пальцам', 'полу', 'ладони', 'борьбе', 'борьбе', 'имени', 'счету', 'наследству', 'возрасту', 'тропинке', 'заду', 'полкам', 'шепоту', 'службе', 'обезвреживанию', 'долине', 'долине', 'проходу', 'проходу', 'адресу', 'бетону', 'порядку', 'волосам', 'улице', 'Портленд', 'соседству', 'Сеньке', 'домам', 'фамилии']\n",
            "['укладываниюрюкзаков']\n",
            "[]\n",
            "['стол', 'руки', 'оружие', 'угол', 'стену', 'стену', 'шиворот', 'полчаса', 'пределы', 'счет', 'черту', 'линию', 'план', 'воздух', 'бесценок', 'горизонт', 'Гостью', 'дело', 'угол', 'угол', 'угол', 'тесты']\n",
            "['спиной', 'окном', 'спиной', 'дверью', 'садом', 'порогом', 'сантиметром', 'Гостьей', 'границей', 'шумом', 'границей', 'исключением', 'стопкой', 'шагом', 'шагом', 'спиной', 'голосом', 'привидениями', 'столиком', 'стеклами', 'девушкой', 'столиком', 'столом']\n",
            "['пользу', 'Энтони', 'место', 'дверь', 'призраков', 'кухню', 'плиту', 'стол', 'стол', 'прогулку', 'стол', 'термометр', 'папоротник', 'секунду', 'эфес', 'секунду', 'пол', 'градусник', 'эфес', 'пол', 'человека', 'освобождение', 'призрак', 'колени', 'глаза', 'ноги', 'пол', 'задание', 'Гостью', 'время', 'метр', 'место', 'кухню', 'пол', 'стену', 'стену', 'куски', 'уши', 'пол', 'дверцу', 'корточки', 'корточки', 'ноги', 'мгновение', 'ноги', 'лицо', 'пол', 'корточки', 'ветру', 'книги', 'стол', 'ноги', 'Локвуда', 'лицо', 'секунду', 'окно', 'стол', 'стол', 'подоконник', 'север', 'плечи', 'поиски', 'глаза', 'место', 'глаза', 'шею', 'ночь', 'подоконник', 'жизнь', 'работу', 'дорогу', 'дорогу', 'встречу', 'гробовщика', 'дежурство', 'дежурство', 'задание', 'ребенка', 'мельницу', 'землю', 'пояс', 'секунду', 'Риджент', 'сегодня', 'минуту', 'сегодня', 'секунду', 'улицу', 'собеседование']\n",
            "['лицах', 'поясах', 'лице', 'лице', 'улицах', 'улицах', 'пороге', 'стене', 'лестнице', 'поясе', 'полу', 'клочках', 'липучке', 'столе', 'столе', 'копне', 'дому', 'свете', 'столе', 'кухне', 'столе', 'подносе', 'руках', 'кухне', 'окнах', 'кухне', 'столике', 'фотографии', 'фотографии', 'столе', 'куртке', 'кухне', 'стенах', 'полу', 'кровати', 'месте', 'кухне', 'свете', 'поясе', 'Гостье', 'ткани', 'поясе', 'месте', 'коже', 'полуслове', 'поясе', 'поясе', 'груди', 'небе', 'обоях', 'конце', 'порогах', 'штукатурке', 'лбу', 'столе', 'площадке', 'краю', 'ногах', 'спине', 'коленях', 'поясе', 'шее', 'юге', 'севере', 'окраине', 'улице', 'склоне', 'месте', 'тропинке', 'задании', 'складе', 'основании', 'мельнице', 'полпути', 'небе', 'двери', 'стенах', 'траве', 'мельнице', 'месте', 'улице', 'асфальте', 'ветру', 'углу', 'углу', 'деревьях', 'поясе', 'полках', 'парне', 'животе', 'столе', 'каблуках', 'столе', 'улице', 'дотации']\n",
            "['колокольчик', 'перчатку', 'дом', 'карман', 'время', 'тишину', 'сторону', 'замок', 'дом', 'спину', 'дом', 'темноту', 'мрак', 'темноту', 'время', 'минуты', 'сторону', 'карман', 'слово', 'чай', 'стопку', 'дом', 'холл', 'стену', 'темноту', 'пар', 'коридор', 'масло', 'беду', 'время', 'сторону', 'сторону', 'сторону', 'холод', 'мороз', 'свинарник', 'рамку', 'сторону', 'воду', 'время', 'ответ', 'спальню', 'погоню', 'отличие', 'руки', 'порядок', 'время', 'карманы', 'ремешок', 'агентство', 'ладоши', 'пыль', 'труху', 'штукатурку', 'груду', 'древесину', 'ответ', 'воздух', 'ответ', 'кучу', 'воздух', 'лицо', 'сторону', 'сторону', 'отверстие', 'стороны', 'объятия', 'кокон', 'карман', 'прямоугольник', 'спину', 'времена', 'школу', 'кресло', 'экран', 'день', 'сторону', 'камыши', 'воду', 'призрак', 'город', 'силу', 'время', 'глубь', 'сторону', 'суд', 'рюкзак', 'Лондон', 'ряд', 'зеркальце', 'колокольчик', 'землю', 'дверь']\n",
            "['глубине', 'доме', 'законе', 'доме', 'тумане', 'лучах', 'тумане', 'доме', 'темноте', 'темноте', 'доме', 'мозгу', 'памяти', 'полумраке', 'руке', 'груди', 'порядке', 'темноте', 'темноте', 'ушах', 'доме', 'холле', 'пределах', 'углах', 'холле', 'Кенте', 'городах', 'Кенте', 'Кенте', 'доме', 'качестве', 'доме', 'доме', 'доме', 'тылу', 'столовой', 'темноте', 'саду', 'темноте', 'луче', 'воздухе', 'комнате', 'туалете', 'середине', 'руках', 'доме', 'груди', 'ковре', 'тени', 'ушах', 'теории', 'кулаке', 'борьбе', 'ушах', 'воздухе', 'плече', 'порядке', 'принципе', 'порядке', 'доме', 'большинстве', 'порядке', 'центре', 'середине', 'доме', 'результате', 'середине', 'темноте', 'дымоходе', 'садах', 'Институте', 'руках', 'панировке', 'комнате', 'порядке', 'стене', 'опасности', 'комнате', 'ушах', 'доске', 'комнате', 'доме', 'агентстве', 'ладони', 'ладони', 'углах', 'штукатурке', 'ладони', 'помещениях', 'тени', 'воздухе', 'дыму', 'стене', 'стене', 'стене', 'руке', 'воздухе', 'ушах', 'доме', 'доме', 'реальности', 'огне', 'дымоходе', 'саду', 'порядке', 'дымоходе', 'порядке', 'дымоходе', 'призраках', 'Афинах', 'цепи', 'цепи', 'прошлом', 'истории', 'Афинах', 'Лондоне', 'Кенте', 'случае', 'случае', 'Лондоне', 'жизни', 'соответствии', 'парке', 'юности', 'Ньюкастле', 'камышах', 'городе', 'камышах', 'главе', 'отеле', 'семье', 'компании', 'случае', 'агентстве', 'темноте', 'стороне', 'деревьях', 'сторонке', 'воздухе', 'темноте', 'темноте', 'деревьях', 'руках', 'темноте', 'темноте', 'проходе', 'темноте', 'постели', 'поезде', 'двери', 'Лондоне', 'виде', 'руке', 'человеке', 'кресле', 'собеседовании', 'груди', 'комнате', 'действии', 'ДЕПИК']\n",
            "['крыльца', 'пола', 'лестницы', 'лестницы', 'лестницы', 'лестницы', 'постели', 'лестницы', 'девушки', 'лестницы', 'лестницы', 'места', 'Мэри', 'Мэри', 'Мэри', 'порога', 'ног', 'улицы', 'колонн', 'минуты']\n",
            "['минуту']\n",
            "['Локвудом', 'улыбкой', 'Локвудом', 'Локвудом', 'Локвудом', 'мистером', 'грохо', 'чаем', 'чаем', 'печеньем', 'Локвудом', 'удовольствием', 'мужем', 'тоской', 'Локвудом', 'Локвудом', 'Локвудом', 'Локвудом', 'Локвудом', 'удовольствием', 'опилками', 'Гостьей', 'усилием', 'юбкой', 'опилками', 'Локвудом', 'опилками', 'парой', 'Локвудом', 'миром', 'Гостями', 'личностью', 'Локвудом', 'зубчиками', 'Локвудом', 'логотипом', 'шелестом', 'Локвудом', 'Гостьей', 'клубами', 'проломом', 'защелкой', 'Локвудом', 'наслаждением', 'призраками', 'призраками', 'Ужасом', 'Фантомом', 'призраками', 'призраками', 'Гостями', 'лавандой', 'Проблемой', 'сестрой', 'трудом', 'Мэри', 'нетерпением', 'агентом', 'наступлением', 'ранцами', 'донесением', 'Якобсом', 'Гостем', 'рапирами', 'грохотом', 'Якобсом', 'именами', 'предложениями', 'колоннами', 'надписью', 'грустью', 'пончиками', 'подставкой', 'грохотом', 'чаем', 'удивлением', 'Гостями']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# номер 62 по Шин Роуд\n",
        "# укладываниюрюкзаков - оно и в тексте слитно"
      ],
      "metadata": {
        "id": "ayFYkPtEd-li"
      },
      "id": "ayFYkPtEd-li",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ДЗ Задача 2 (3 балла).\n",
        "\n",
        "Доработайте задачу с классификацией по темам из предыдущего домашнего задания, заменив стемминг лемматизацией."
      ],
      "metadata": {
        "id": "fIEmXf3xruLI"
      },
      "id": "fIEmXf3xruLI"
    },
    {
      "cell_type": "code",
      "source": [
        "# она была у меня уже с лемматизацией с использованием nltk в 3 домашке, прицеплю сюда еще раз на всякий случай\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "5fxUGG1oryPA"
      },
      "id": "5fxUGG1oryPA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def text_base(texts_list):\n",
        "    text_base = {}\n",
        "\n",
        "    pattern = r'/([^/]+)\\.txt$'\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    for textt in texts_list:\n",
        "        with open(textt, encoding='utf8') as f:\n",
        "            text = f.read()\n",
        "            token_text = word_tokenize(text.lower())\n",
        "            lemmas = []\n",
        "            for t in token_text:\n",
        "                lemmas.append(lemmatizer.lemmatize(t))\n",
        "            filtered_lemmas = [w for w in lemmas if not w.lower() in stop_words]\n",
        "        match = re.search(pattern, textt)\n",
        "        text_base[match.group(1)] = filtered_lemmas\n",
        "\n",
        "    return text_base"
      ],
      "metadata": {
        "id": "akVcsVSqsoRb"
      },
      "id": "akVcsVSqsoRb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_base = text_base(['/content/christmas.txt', '/content/dogs.txt', '/content/ecology.txt', '/content/india.txt', '/content/programming.txt'])"
      ],
      "metadata": {
        "id": "P-YgJdIosrw7"
      },
      "id": "P-YgJdIosrw7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def theme_extractor(file_path, my_base):\n",
        "    my_base = my_base\n",
        "\n",
        "    # открываем файл\n",
        "    with open(file_path, encoding='utf8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # токенизируем\n",
        "    token_text = word_tokenize(text.lower())\n",
        "    # лемматизируем\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for t in token_text:\n",
        "        lemmas.append(lemmatizer.lemmatize(t))\n",
        "\n",
        "    # удаляем стоп-слова\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_lemmas = [w for w in lemmas if not w.lower() in stop_words]\n",
        "\n",
        "    # сравниваем с текстами из базы\n",
        "    my_count = 0 # наибольшее кол-во общих слов\n",
        "    my_topic = 'nothing' # название текста с наибольшим кол-вом общих слов\n",
        "    for k, v in my_base.items():\n",
        "        common_words = list(filter(lambda word: word in v, filtered_lemmas))\n",
        "        number_of_common_words = len(common_words)\n",
        "        if my_count < number_of_common_words:\n",
        "            my_count = number_of_common_words\n",
        "            my_topic = k\n",
        "\n",
        "    return my_topic, my_count"
      ],
      "metadata": {
        "id": "FznhH0dqswcD"
      },
      "id": "FznhH0dqswcD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "theme_extractor('/content/test_spaniel.txt', my_base)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrB5cXx9sw_L",
        "outputId": "6ef220a1-6b0b-4b00-dc92-3ffde0ad906c"
      },
      "id": "DrB5cXx9sw_L",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('dogs', 141)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "theme_extractor('/content/test_sea_pollution.txt', my_base)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiEL0rt7s0CM",
        "outputId": "909f8c1a-7465-4cfc-d296-0567dedf1f1f"
      },
      "id": "wiEL0rt7s0CM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ecology', 43)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}